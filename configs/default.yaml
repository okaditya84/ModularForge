# =============================================================================
# ModularForge — Default Configuration (50M PoC)
# =============================================================================
# This configuration trains a ~50M parameter Mixture-of-Experts model
# from 5 independently trained expert modules on WikiText-103.
#
# Hardware: Apple M4 Pro MacBook / Kaggle GPU / Any machine with >= 4GB RAM
# Expected training time: ~2-4 hours per expert on CPU, ~30 min on GPU
# Peak memory: < 200MB during training, < 100MB during assembly
# =============================================================================

model:
  d_model: 512 # Internal representation width
  n_heads: 8 # Attention heads (512/8 = 64 dims each)
  n_layers: 4 # Transformer blocks deep
  d_ff: 2048 # Expert FFN hidden dimension (4× d_model)
  n_experts: 5 # Number of expert modules
  top_k: 2 # Experts activated per token
  vocab_size: 16384 # BPE vocabulary size
  max_seq_len: 512 # Maximum context length
  dropout: 0.1 # General dropout
  expert_dropout: 0.1 # Expert-specific dropout

training:
  learning_rate: 3.0e-4 # AdamW learning rate
  batch_size: 16 # Sequences per training step
  epochs_shared: 3 # Epochs for shared component training
  epochs_expert: 5 # Epochs per expert module
  warmup_steps: 200 # LR warmup steps
  weight_decay: 0.01 # L2 regularization
  max_grad_norm: 1.0 # Gradient clipping
  gradient_accumulation_steps: 1
  seed: 42 # Reproducibility seed
  num_workers: 4 # Data loading workers (set to 0 on Kaggle)
  use_amp: false # Mixed precision (enable on GPU)
  device: "auto" # auto / cpu / mps / cuda
  checkpoint_every: 1000 # Steps between checkpoints
  log_every: 50 # Steps between log prints
  eval_every: 500 # Steps between evaluations

data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-103-raw-v1"
  tokenizer_vocab_size: 16384
  partition_strategy: "clustered" # random / clustered / curriculum
  overlap_ratio: 0.1 # 10% data overlap between partitions
  min_article_length: 100 # Minimum chars per article
  data_dir: "data"
  max_articles: null # null = use all articles

assembly:
  output_format: "safetensors"
  router_init: "kaiming" # uniform / kaiming / data_stats
  calibrate: true
  calibration_samples: 1000
  output_dir: "outputs"

evaluation:
  eval_batch_size: 32
  generate_samples: 5
  generate_max_tokens: 200
  temperature: 0.8
  top_k: 50
  top_p: 0.9
  prompts:
    - "The scientific method is"
    - "In mathematics, a prime number"
    - "The history of computing began"
    - "Machine learning algorithms can"
    - "The structure of DNA was"
